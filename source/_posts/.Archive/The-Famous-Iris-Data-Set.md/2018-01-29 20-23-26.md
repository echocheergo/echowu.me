---
title: Data Scientist Journey --- The Famous Iris Data Set
date: 2018-01-24 20:55:41
tags: data-scientist-journey, data-visualization
---
# The Famous Iris Data Set
This is the first blog where I write about my journey of becoming a data scientist. Today we'll talk about how we can analyze and visualize the famous Iris dataset.

> The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.

The Iris dataset is commonly used as a "introductory" dataset for evaluating various data analysis techniques such as SVM, KNN, decision tree, etc. In this post, different measures are taken to visualize the dataset and perform multi-class classification on it.

## Get the data
The first thing I want to do is get the data and take a look at how it distributes. Conveniently, *sklearn* already provides the Iris dataset along with the package.
```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn import datasets

iris_dataset = datasets.load_iris()
X = iris_dataset.data
Y = iris_dataset.target

iris_dataframe = pd.DataFrame(X, columns=iris_dataset.feature_names)
# create a scatter matrix from the dataframe, color by y_train
grr = pd.plotting.scatter_matrix(iris_dataframe, c=Y, figsize=(15, 15), marker='o',
                                 hist_kwds={'bins': 20}, s=60, alpha=.8)
```
Once we get the dataset, we do a quick scatter matrix plot to see how the data looks like.

![](./_image/Screen Shot 2018-01-29 at 5.06.34 PM.png)
[Scatter-Matrix Plot]
As can be seen from the scatter plot, the three species of flowers are quite distinguishable on a few dimensions, including **petal width**, **petal length**, while two species are more alike each other than the third one.

## KDE Plot
One common way to visualize data is **kernel density estimation**, which is **a non-parametric way to estimate the probability density function of a random variable**. Basically it gives people an idea about the distribution of data by smoothing them in a cerntain interval. \

```python
import seaborn as sns
f, ax = plt.subplots(figsize=(8,8))
setosa = X[Y==0]
versicolor = X[Y==1]
virginica = X[Y==2]
ax = sns.kdeplot(setosa[:,1], setosa[:,2], cmap="Greens", shade=True, shade_lowest=False)
ax = sns.kdeplot(versicolor[:,1], versicolor[:,2], cmap="Reds", shade=True, shade_lowest=False)
ax = sns.kdeplot(virginica[:,1], virginica[:,2], cmap="Blues", shade=True, shade_lowest=False)
red = sns.color_palette("Reds")[-2]
blue = sns.color_palette("Blues")[-2]
green = sns.color_palette("Greens")[-2]
ax.text(2.5, 7.2, "virginica", size=16, color=blue)
ax.text(1.5, 4.2, "versicolor", size=16, color=red)
ax.text(3.8, 2.1, "setosa", size=16, color=green)
```
The seaborn package generates a nice looking KDE plot.

![](./_image/Screen Shot 2018-01-29 at 5.29.24 PM.png)

## PCA on the data
Note that in the above plots we are only considering two dimensions of the data per plot. In reality, we want to examine all the dimensions available to make accurate preidiction. The trick to do this is called **Principal Component Analysis**, which defines an ***orthorgonal linear transformation*** that transforms data into a lower dimension. This helps us visualize the dataset as a whole and has some other [implications](https://en.wikipedia.org/wiki/Principal_component_analysis#First_component) as well.

```python
# import some data to play with
iris = datasets.load_iris()

# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
X_reduced = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,
           cmap=plt.cm.Set1, edgecolor='k', s=40)
ax.set_title("First three PCA directions")
ax.set_xlabel("1st eigenvector")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel("2nd eigenvector")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel("3rd eigenvector")
ax.w_zaxis.set_ticklabels([])

plt.show()
```
and this would gives us something like below

![](./_image/Screen Shot 2018-01-29 at 5.42.49 PM.png)
[PCA components visualzation of the Iris dataset]
With the PCA components available, we can even plot a KDE on the PCA components, 
 
![](./_image/Screen Shot 2018-01-29 at 7.18.01 PM.png)
[KDE plot of PCA components]
## SVM Hyperplane
Now, only being able to visualize the data is clearly not enough. Since this is a classification problem, I'll try to use SVM to classify the Iris dataset and draw the hyperplanes between different classes.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
%matplotlib inline
X = iris.data[:, -2:]
Y = iris.target
res = 0.01

# model title
titles = ['SVM with linear kernel',
         'SVM with rbf kernel',
         'SVM with poly kernel',
         'LinearSVC']

# model definition
svc1 = svm.SVC(kernel='linear', C=1.).fit(X, Y)
svc2 = svm.SVC(kernel='rbf', C=1., gamma=0.7).fit(X,Y)
svc3 = svm.SVC(kernel='poly', C=1., degree=3).fit(X,Y) 
svc4 = svm.LinearSVC(C=1.).fit(X,Y)

# create mesh
min_x, max_x = min(X[:,0]) - 1, max(X[:,0]) + 1
min_y, max_y = min(X[:,1]) - 1, max(X[:,1]) + 1
x_mesh, y_mesh = np.meshgrid(np.arange(min_x, max_x, res),
                            np.arange(min_y, max_y, res))

# start plotting
plt.figure(1, figsize=(10,10))
for i, clf in enumerate((svc1, svc2, svc3, svc4)):
    plt.subplot(2,2,i+1)
    plt.subplots_adjust(wspace=0.4, hspace=0.4)
    
    p = clf.predict(np.c_[x_mesh.ravel(), y_mesh.ravel()])

    plt.contourf(x_mesh, y_mesh, p.reshape(x_mesh.shape), cmap=plt.cm.Paired, alpha=0.8)
    
    plt.scatter(X[:,0], X[:,1], c=Y*100, cmap=plt.cm.Paired, edgecolors='b')
    plt.xlabel('Sepal length')
    plt.ylabel('Sepal width')
    plt.xlim(x_mesh.min(), x_mesh.max())
    plt.ylim(y_mesh.min(), y_mesh.max())
    plt.xticks(())
    plt.yticks(())
    plt.title(titles[i])
```
The script above generates a figure consisting of four different plots, corresponding to different SVM model used.

![](./_image/Screen Shot 2018-01-29 at 8.21.52 PM.png)
[SVM ]