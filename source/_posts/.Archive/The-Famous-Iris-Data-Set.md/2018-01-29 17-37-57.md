---
title: Data Scientist Journey --- The Famous Iris Data Set
date: 2018-01-24 20:55:41
tags: data-scientist-journey, data-visualization
---
# The Famous Iris Data Set
This is the first blog where I write about my journey of becoming a data scientist. Today we'll talk about how we can analyze and visualize the famous Iris dataset.

> The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.

The Iris dataset is commonly used as a "introductory" dataset for evaluating various data analysis techniques such as SVM, KNN, decision tree, etc. In this post, different measures are taken to visualize the dataset and perform multi-class classification on it.

## Get the data
The first thing I want to do is get the data and take a look at how it distributes. Conveniently, *sklearn* already provides the Iris dataset along with the package.
```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn import datasets

iris_dataset = datasets.load_iris()
X = iris_dataset.data
Y = iris_dataset.target

iris_dataframe = pd.DataFrame(X, columns=iris_dataset.feature_names)
# create a scatter matrix from the dataframe, color by y_train
grr = pd.plotting.scatter_matrix(iris_dataframe, c=Y, figsize=(15, 15), marker='o',
                                 hist_kwds={'bins': 20}, s=60, alpha=.8)
```
Once we get the dataset, we do a quick scatter matrix plot to see how the data looks like.

![](./_image/Screen Shot 2018-01-29 at 5.06.34 PM.png)
[Scatter-Matrix Plot]
As can be seen from the scatter plot, the three species of flowers are quite distinguishable on a few dimensions, including **petal width**, **petal length**, while two species are more alike each other than the third one.

## KDE Plot
One common way to visualize data is **kernel density estimation**, which is **a non-parametric way to estimate the probability density function of a random variable**. Basically it gives people an idea about the distribution of data by smoothing them in a cerntain interval. \

```python
import seaborn as sns
f, ax = plt.subplots(figsize=(8,8))
setosa = X[Y==0]
versicolor = X[Y==1]
virginica = X[Y==2]
ax = sns.kdeplot(setosa[:,1], setosa[:,2], cmap="Greens", shade=True, shade_lowest=False)
ax = sns.kdeplot(versicolor[:,1], versicolor[:,2], cmap="Reds", shade=True, shade_lowest=False)
ax = sns.kdeplot(virginica[:,1], virginica[:,2], cmap="Blues", shade=True, shade_lowest=False)
red = sns.color_palette("Reds")[-2]
blue = sns.color_palette("Blues")[-2]
green = sns.color_palette("Greens")[-2]
ax.text(2.5, 7.2, "virginica", size=16, color=blue)
ax.text(1.5, 4.2, "versicolor", size=16, color=red)
ax.text(3.8, 2.1, "setosa", size=16, color=green)
```
The seaborn package generates a nice looking KDE plot.

![](./_image/Screen Shot 2018-01-29 at 5.29.24 PM.png)

## PCA on the data
Note that in the above plots we are only considering two dimensions of the data per plot. In reality, we want to examine all the dimensions available to make accurate preidiction. The trick to do this is called **Principal Component Analysis**, which 